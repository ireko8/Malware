import keras.layers as kl
from keras.models import Model
from keras.optimizers import Adam
from .embedding import embedding, simple_embedding
from .embedding import tosh_embedding, fm_embedding


def embedded_mlp(embed_conf, embedding_trainable=True):
    """Multi-layer Perceptron for embedded data
    """

    input_layer, embedded_layer = embedding(embed_conf,
                                            embedding_trainable)
    x = kl.concatenate(embedded_layer)
    x = kl.Dropout(0.2)(x)

    x = kl.Dense(64, init='he_normal')(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.2)(x)

    x = kl.Dense(16, init='he_normal')(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.2)(x)

    outputs = kl.Dense(1, activation='sigmoid', init='he_normal')(x)
    model = Model(input_layer, outputs, name="MLP")
    opt = Adam(amsgrad=True)
    model.compile(
        optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    return model


def embedded_mlp_multi(embed_conf, out_size):
    """Multi-layer Perceptron for embedded data
    """

    input_layer, embedded_layer = embedding(embed_conf)
    x = kl.concatenate(embedded_layer)
    x = kl.Dropout(0.2)(x)

    x = kl.Dense(64, init='he_normal')(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.2)(x)

    x = kl.Dense(16, init='he_normal')(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.2)(x)

    outputs = kl.Dense(out_size, activation='softmax', init='he_normal')(x)
    model = Model(input_layer, outputs, name="MLP")
    opt = Adam(amsgrad=True)
    model.compile(
        optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model


def deep_fm(fm_conf):
    """Multi-layer Perceptron for embedded data
    """

    input_layer, fm_layer = fm_embedding(fm_conf)
    x = kl.concatenate(fm_layer)
    x = kl.Dropout(0.2)(x)

    x = kl.Dense(64)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.2)(x)

    x = kl.Dense(16)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.2)(x)

    outputs = kl.Dense(1, activation='sigmoid')(x)
    model = Model(input_layer, outputs, name="FM")
    opt = Adam(amsgrad=True)
    model.compile(
        optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
    return model


def simple_embedded_mlp(embed_conf):
    """Multi-layer Perceptron for embedded data
    """

    input_layer, embedded_layer = simple_embedding(embed_conf)
    x = kl.concatenate(embedded_layer)
    x = kl.Dropout(0.5)(x)

    x = kl.Dense(256)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.5)(x)

    x = kl.Dense(64)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.5)(x)

    outputs = kl.Dense(1, activation='sigmoid')(x)
    model = Model(input_layer, outputs, name="MLP")
    model.compile(
        optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


def tosh_nn(embed_conf):
    """MLP based on tosh's NN."""
    input_layer, table_layer = tosh_embedding(embed_conf)
    tab = kl.concatenate(table_layer)

    fc1 = kl.Dense(512)(tab)
    fc1 = kl.advanced_activations.PReLU()(fc1)
    fc1 = kl.BatchNormalization()(fc1)
    fc1 = kl.Dropout(0.5)(fc1)

    fc2 = kl.concatenate([tab, fc1])
    fc2 = kl.Dense(512)(fc2)
    fc2 = kl.advanced_activations.PReLU()(fc2)
    fc2 = kl.BatchNormalization()(fc2)
    fc2 = kl.Dropout(0.5)(fc2)

    fc2 = kl.Dense(64)(fc2)
    fc2 = kl.advanced_activations.PReLU()(fc2)
    fc2 = kl.BatchNormalization()(fc2)
    fc2 = kl.Dropout(0.5)(fc2)

    outputs = kl.Dense(1, activation='sigmoid')(fc2)
    model = Model(input_layer, outputs, name="MLP")
    model.compile(
        optimizer="adam", loss='binary_crossentropy', metrics=['accuracy'])
    return model


def embedding_multi_out(embed_conf):
    """ Multi-output embedding network
    """
    output = list()
    x_in, out_ls = embedding(embed_conf)
    for layer in out_ls:
        x = kl.Dense(16)(layer)
        x = kl.advanced_activations.PReLU()(x)
        x = kl.BatchNormalization()(x)
        x = kl.Dropout(0.2)(x)

        x = kl.Dense(1, activation='sigmoid')(x)
        output.append(x)

    all_out = kl.concatenate(out_ls)

    all_out = kl.Dense(256)(all_out)
    all_out = kl.advanced_activations.PReLU()(all_out)
    all_out = kl.BatchNormalization()(all_out)
    all_out = kl.Dropout(0.5)(all_out)

    all_out = kl.Dense(64)(all_out)
    all_out = kl.advanced_activations.PReLU()(all_out)
    all_out = kl.BatchNormalization()(all_out)
    all_out = kl.Dropout(0.5)(all_out)

    all_out = kl.Dense(1, activation='sigmoid')(all_out)
    output.append(all_out)

    model = Model(x_in, output, name='Multi-in Multi-out')
    model.compile(
        optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


def sparse_mlp(input_shape):
    """sparse MLP
    """
    x_in = kl.Input(shape=(input_shape, ))
    x = kl.Dense(1024, activation='relu')(x_in)
    x = kl.Dropout(0.5)(x)
    x = kl.Dense(512, activation='relu')(x)
    x = kl.Dropout(0.5)(x)
    x = kl.Dense(256, activation='relu')(x)
    x = kl.Dropout(0.5)(x)
    x = kl.Dense(64, activation='relu')(x)
    outputs = kl.Dense(1, activation='sigmoid')(x)
    model = Model(x_in, outputs, name="Sparse_MLP")
    model.compile(
        optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
    return model
