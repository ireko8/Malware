import keras.layers as kl


def simple_embedding(conf):
    """make embedded and input layer
    """
    input_layer = list()
    embedded_layer = list()
    embed_conf, num_conf = conf
    for name, input_dim, output_dim in embed_conf:
        x_in = kl.Input(shape=(1,), dtype='int32')
        embedded = kl.Embedding(input_dim,
                                output_dim,
                                input_length=1)(x_in)
        embedded = kl.Flatten()(embedded)
        embedded = kl.Dense(output_dim)(embedded)
        embedded = kl.advanced_activations.PReLU()(embedded)
        embedded_layer.append(embedded)
        input_layer.append(x_in)

    dense_layer = list()
    for name, setting in num_conf.items():
        x_in = kl.Input(shape=(setting[1],),
                        name=name,
                        dtype='float32')
        dense_layer.append(x_in)
        input_layer.append(x_in)
    
    if len(dense_layer) > 1:
        dense_num = kl.concatenate(dense_layer)
    else:
        dense_num = dense_layer[0]
    dense_num = kl.Dense(10)(dense_num)
    dense_num = kl.advanced_activations.PReLU()(dense_num)
    embedded_layer.append(dense_num)
    
    return input_layer, embedded_layer


def tosh_embedding(conf):
    """make embedded and input layer tosh version
    """
    input_layer = list()
    embedded_layer = list()
    embed_conf, num_conf = conf

    # embedding
    for name, input_dim, output_dim in embed_conf:
        x_in = kl.Input(shape=(1,),
                        name=name,
                        dtype='int32')
        embedded = kl.Embedding(input_dim,
                                output_dim,
                                input_length=1)(x_in)
        embedded = kl.Dropout(0.25)(embedded)
        embedded = kl.Flatten()(embedded)
        embedded_layer.append(embedded)
        input_layer.append(x_in)

    x = kl.concatenate(embedded_layer)
    x = kl.Dense(128)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.5)(x)
    x = kl.concatenate(embedded_layer)
    x = kl.Dense(64)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    emb_out = kl.Dropout(0.75)(x)
    
    table_layers = [emb_out]
    for tab_name, setting in num_conf.items():
        x_in = kl.Input(shape=(setting[1],),
                        name=tab_name,
                        dtype='float32')
        
        dense_num = kl.Dense(setting[2])(x_in)
        dense_num = kl.advanced_activations.PReLU()(dense_num)
        dense_num = kl.BatchNormalization()(dense_num)
        dense_num = kl.Dropout(0.5)(dense_num)
        dense_num = kl.Dense(setting[2]//2)(x_in)
        dense_num = kl.advanced_activations.PReLU()(dense_num)
        dense_num = kl.BatchNormalization()(dense_num)
        dense_num = kl.Dropout(0.75)(dense_num)

        input_layer.append(x_in)
        table_layers.append(dense_num)

    return input_layer, table_layers


def embedding(conf):
    """make embedded and input layer
    """
    input_layer = list()
    out_layer = list()
    embedded_layer = list()
    embed_conf, num_conf = conf
    for name, input_dim, output_dim in embed_conf:
        x_in = kl.Input(shape=(1,),
                        name=name,
                        dtype='int32')
        embedded = kl.Embedding(input_dim,
                                output_dim,
                                input_length=1)(x_in)
        embedded = kl.Flatten()(embedded)
        embedded_layer.append(embedded)
        input_layer.append(x_in)

    x = kl.concatenate(embedded_layer)
    x = kl.Dense(64)(x)
    x = kl.advanced_activations.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.1)(x)
    emb_out = x

    dense_layer = list()
    for name, setting in num_conf.items():
        x_in = kl.Input(shape=(setting[1],),
                        name=name,
                        dtype='float32')
        dense_num = kl.Dense(setting[2])(x_in)
        dense_num = kl.advanced_activations.PReLU()(dense_num)
        dense_num = kl.BatchNormalization()(dense_num)
        dense_num = kl.Dropout(0.1)(dense_num)
        input_layer.append(x_in)
        dense_layer.append(dense_num)

    out_layer = dense_layer + [emb_out]

    return input_layer, out_layer
