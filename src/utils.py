import resource
import copy
from pathlib import Path
from datetime import datetime
from shutil import copytree
import random
import numpy as np
import torch
import feather
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from config import conf


def set_ulimit(limit):
    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)
    resource.setrlimit(resource.RLIMIT_NOFILE, (limit, rlimit[1]))


def now():
    return datetime.now().strftime("%Y_%m_%d_%H_%M_%S")


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = False
    torch.manual_seed(seed)


def load_csv(debug=False, test=False):
    if debug:
        load_rows = 1000
    else:
        load_rows = None

    dtypes = copy.deepcopy(conf.dtypes)
    if test:
        path = conf.TEST
        dtypes.pop('HasDetections')
    else:
        path = conf.TRAIN
        
    df = pd.read_csv(path, dtype=dtypes, nrows=load_rows)
    return df


def load_feather(test=False):
    if test:
        path = 'features/test.ftr'
    else:
        path = 'features/train.ftr'

    df = feather.read_dataframe(path)
    return df


def load_fs(test=False):
    if test:
        path = 'features/test/'
    else:
        path = 'features/train/'
    fs_list = Path(path).glob('*.ftr')
    fs = list()
    for c in tqdm(fs_list):
        if c.stem in conf.features:
            f = feather.read_dataframe(c)
            fs.append(f)
    df = pd.concat(fs, axis=1)
    return reduce_mem_usage(df)


def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'int' or col_type == 'float':
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    # print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    # print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return df


def plot_importance(fi, fn, img_path):
    if isinstance(fi, list):
        fi_mean = np.mean(fi, axis=0)
        fi_std = np.std(fi, axis=0)
        fi_df = pd.DataFrame(np.array([fi_mean, fi_std, fn]).T,
                             columns=['feature_importance', 'fi_std', 'name'])
    
        fi_df.feature_importance = fi_df.feature_importance.astype('float32')
        fi_df.fi_std = fi_df.fi_std.astype('float32')
        
        fi_df = fi_df.sort_values('feature_importance', ascending=True)
        fi_df.plot.barh(y='feature_importance', x='name', yerr='fi_std')
    else:
        fi_df = pd.DataFrame(np.array([fi, fn]).T,
                             columns=['feature_importance', 'name'])
        fi_df.feature_importance = fi_df.feature_importance.astype('float32')
        fi_df = fi_df.sort_values('feature_importance', ascending=True)
        fi_df.plot.barh(y='feature_importance', x='name')
        fi_df.to_csv(img_path / 'fi.csv', index=False)
        import ipdb
        ipdb.set_trace()

    plt.savefig(img_path / 'fi_plot.pdf')
    return fi_df


def rank_average(prob_list):
    avg = np.zeros_like(prob_list[0])
    for p in prob_list:
        p = pd.Series(p)
        avg += p.rank() / p.shape[0]
    avg /= len(prob_list)
    return avg


def count_parameter(model):
    return sum(p.numel() for p in model.parameters())


def get_lr(optimizer):
    lr = list()
    for param_group in optimizer.param_groups:
        lr.append(param_group['lr'])
    if len(lr) == 1:
        return lr[0]
    else:
        return lr


def copy_script(cv_path):
    copytree('src', cv_path / 'src')


def compare_submission(preds):
    subs = Path('submissions').glob('*.csv.gz')
    import ipdb
    with ipdb.launch_ipdb_on_exception():
        for s in subs:
            other_pred = pd.read_csv(s, compression='gzip')
            corr = np.corrcoef(other_pred.HasDetections,
                               preds)
            assert(corr[0, 1] < 0.99)
            print(s, corr[0, 1])


def make_submission(preds, fname, compare=True):
    sub = pd.read_csv('input/sample_submission.csv')
    if compare:
        compare_submission(preds)
    sub['HasDetections'] = preds
    sub.to_csv(fname, index=False, compression='gzip')
