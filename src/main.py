from pathlib import Path
import lightgbm
import numpy as np
from config import conf
from sklearn.model_selection import StratifiedKFold
from utils import load_csv, now, plot_importance, rank_average
from utils import make_submission
from logger import Logger


def main():
    experiment_name = now()
    cv_path = Path(f'result/{experiment_name}')
    cv_path.mkdir(parents=True)

    log = Logger(experiment_name, cv_path / 'exp.log')

    log.info('load data')
    with log.interval_timer('load data'):
        train = load_csv(debug=conf.debug)
        train = train.drop(conf.drop_cols, axis=1)
        test = load_csv(debug=conf.debug, test=True)
        test = test.drop(conf.drop_cols, axis=1)

    cv = StratifiedKFold(n_splits=5, shuffle=True,
                         random_state=conf.seed)

    train_X = train.loc[:, train.columns != 'HasDetections']
    train_y = train.loc[:, 'HasDetections']
    train_set = lightgbm.Dataset(train_X, label=train_y)

    log.info('learning start')
    log.double_kiritori()
    model, res = lightgbm.cv(conf.params, train_set,
                             num_boost_round=conf.num_boost_round,
                             folds=cv.split(train_X, train_y),
                             verbose_eval=conf.verbose_eval,
                             seed=conf.seed)
    log.double_kiritori()
    log.info('done')

    val_preds = np.zeros_like(train_y, dtype='float32')
    for i, (_, test_ind) in enumerate(cv.split(train_X, train_y)):
        val_preds[test_ind] = model.boosters[i].predict(train_X.loc[test_ind])
    np.save(cv_path / 'val_preds.npy', val_preds)

    fi_gain = model.feature_importance(importance_type='gain')
    fn = train_set.feature_name
    plot_importance(fi_gain, fn, cv_path)

    preds = model.predict(test)
    test_preds = rank_average(preds)
    np.save(cv_path / 'test_preds.npy', test_preds)

    make_submission(test_preds, f'submissions/{experiment_name}.csv')


if __name__ == '__main__':
    main()
