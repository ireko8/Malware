import numpy as np
import pandas as pd
import keras.layers as kl
from keras import Model
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.regularizers import l2
from keras.optimizers import Adam
from nn_utils.neural import df_to_list
from .embedding import embedding_for_ae
from config import conf


def DAE(num_size, ls):
    # denoising autoencoder
    
    x_in = kl.Input(shape=(num_size,), dtype="float32")

    e1 = kl.Dense(ls)(x_in)  # 1500 original
    e1 = kl.Activation('relu', name='e1')(e1)
    # x = kl.BatchNormalization()(x)
    # x = kl.Dropout(0.2)(x)

    e2 = kl.Dense(ls)(e1)  # 1500 original
    e2 = kl.Activation('relu', name='e2')(e2)
    # encode = kl.BatchNormalization()(encode)
    # encode = kl.Dropout(0.2)(encode)

    e3 = kl.Dense(ls)(e2)  # 1500 original
    e3 = kl.Activation('relu', name='e3')(e3)
    # x = kl.BatchNormalization()(x)
    # x = kl.Dropout(0.2)(x)

    outputs = kl.Dense(num_size)(e3)
    dae = Model(inputs=x_in, outputs=outputs)
    opt = Adam(amsgrad=True)
    dae.compile(optimizer=opt, loss='mse')

    return dae


def tosh_DAE(nn_conf):
    outputs_num = []
    outputs_cat = []
    hls = conf.ae_hls

    inputs, out_layer = embedding_for_ae(nn_conf, hls)

    e1 = kl.concatenate(out_layer)
    # e1 = out_layer[-1]
    e1 = kl.Dense(hls)(e1)  # 1500 original
    e1 = kl.Activation('relu', name='e1')(e1)

    e2 = kl.Dense(hls)(e1)  # 1500 original
    e2 = kl.Activation('relu', name='e2')(e2)

    e3 = kl.Dense(hls)(e2)  # 1500 original
    e3 = kl.Activation('relu', name='e3')(e3)

    cat_conf, num_conf = nn_conf
    # for name, cat_input_size, _ in cat_conf:
    #     x_cat = kl.Dense(cat_input_size, name=f'{name}_dense')(e3)
    #     outputs_cat.append(x_cat)

    for name, setting in num_conf.items():
        x_num = kl.Dense(setting[1])(e3)
        outputs_num.append(x_num)

    # losses = ['categorical_crossentropy'] * len(outputs_cat)
    losses = ['mse'] * len(outputs_num)

    dae = Model(inputs, outputs_cat + outputs_num)

    opt = Adam(amsgrad=True)
    dae.compile(optimizer=opt, loss=losses)

    return dae


def x_generator(X, batch_size, shuffle=True):
    # batch generator of input
    n = X.shape[0]
    while True:
        base_id = np.random.permutation(n)
        for start in range(0, n, batch_size):
            end = min(start + batch_size, n)
            idx = base_id[start:end]
            batch_x = X.iloc[idx]
            yield batch_x


def mix_generator(X, batch_size, swaprate=0.15, shuffle=True):
    # generator of noized input and output
    # swap 0.15% of values of data with values of another
    gen1 = x_generator(X, batch_size, shuffle)
    num_cols = X.shape[1]
    while True:
        batch1 = next(gen1)
        new_batch = batch1.copy()
        batch1_size = batch1.shape[0]
        num_swap = int(swaprate * batch1_size)
        for i in range(num_cols):
            swap_idx = np.random.choice(
                batch1_size, 2 * num_swap, replace=False)
            new_batch[swap_idx[:num_swap], i] = batch1[swap_idx[num_swap:], i]

        yield (new_batch, batch1)


def mix_generator_v3(X, batch_size, nn_conf, swaprate=0.15, shuffle=True):
    # generator of noized input and output
    # swap 0.15% of values of data with values of another
    cols = X.columns
    n = X.shape[0]
    while True:
        base_id = np.random.permutation(n)
        for start in range(0, n, batch_size):
            end = min(start + batch_size, n)
            idx = base_id[start:end]
            batch1 = X.iloc[idx].reset_index(drop=True)
            batch1_size = batch1.shape[0]
            new_batch = batch1.copy()
            num_swap = int(swaprate * batch1_size)
            for c in cols:
                swap_idx = np.random.choice(
                    batch1_size, 2*num_swap, replace=False)
                mask = new_batch.index.isin(swap_idx[:num_swap])
                mask2 = batch1.index.isin(swap_idx[num_swap:])
                new_batch.loc[mask, c] = batch1.loc[mask2, c].values

            yield (new_batch, batch1)


def get_NN(input_shape, dae):
    l2_loss = l2(0.05)
    for l in dae.layers:
        l.trainable = False
    
    encoders = list()
    for i in range(1, 4):
        encoders.append(dae.get_layer(f'e{i}').output)
    x_in = kl.concatenate(encoders)
    # x = kl.Dropout(0.1)(x_in)

    x = kl.Dense(
        input_shape // 4, kernel_regularizer=l2_loss)(x_in)  # 4500 original
    x = kl.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.5)(x)

    x = kl.Dense(
        input_shape // 4, kernel_regularizer=l2_loss)(x)  # 1000 original
    x = kl.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.5)(x)

    # x = kl.Dense(64, kernel_regularizer=l2_loss)(x)  # 1000 original
    # x = kl.PReLU()(x)
    # x = kl.BatchNormalization()(x)
    # x = kl.Dropout(0.5)(x)

    predictions = kl.Dense(
        1, activation='sigmoid', kernel_regularizer=l2_loss)(x)

    model = Model(inputs=dae.input, outputs=predictions)

    model.compile(
        loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model


def get_NN2(input_shapes):
    l2_loss = l2(0.05)
    # dae.trainable = False

    input_list = []
    agged_list = []

    # x = dae.get_layer("feature").output
    for input_shape in input_shapes:
        x_in = kl.Input((input_shape, ))
        # x = kl.Dropout(0.1)(x_in)

        x = kl.Dense(
            input_shape // 4, kernel_regularizer=l2_loss)(x_in)  # 4500 original
        x = kl.PReLU()(x)
        x = kl.BatchNormalization()(x)
        x = kl.Dropout(0.5)(x)

        x = kl.Dense(
            128, kernel_regularizer=l2_loss)(x_in)  # 4500 original
        x = kl.PReLU()(x)
        x = kl.BatchNormalization()(x)
        x = kl.Dropout(0.5)(x)

        input_list.append(x_in)
        agged_list.append(x)
        

    x = kl.concatenate(agged_list)
    x = kl.Dense(
        256, kernel_regularizer=l2_loss)(x)  # 1000 original
    x = kl.PReLU()(x)
    x = kl.BatchNormalization()(x)
    x = kl.Dropout(0.5)(x)

    predictions = kl.Dense(
        1, activation='sigmoid', kernel_regularizer=l2_loss)(x)

    model = Model(inputs=x_in, outputs=predictions)

    model.compile(
        loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model



def train_generator(x, y, batch_size, shuffle=True):
    batch_index = 0
    n = x.shape[0]
    while True:
        if batch_index == 0:
            index_array = np.arange(n)
        if shuffle:
            index_array = np.random.permutation(n)

        current_index = (batch_index * batch_size) % n
        if n >= current_index + batch_size:
            current_batch_size = batch_size
            batch_index += 1
        else:
            current_batch_size = n - current_index
            batch_index = 0

        batch_x = x[index_array[current_index:current_index +
                                current_batch_size]]
        batch_y = y[index_array[current_index:current_index +
                                current_batch_size]]

        yield batch_x, batch_y


def test_generator(x, batch_size, shuffle=False):
    batch_index = 0
    n = x.shape[0]
    while True:
        if batch_index == 0:
            index_array = np.arange(n)
        if shuffle:
            index_array = np.random.permutation(n)

        current_index = (batch_index * batch_size) % n
        if n >= current_index + batch_size:
            current_batch_size = batch_size
            batch_index += 1
        else:
            current_batch_size = n - current_index
            batch_index = 0

        batch_x = x[index_array[current_index:current_index +
                                current_batch_size]]

        yield batch_x


def get_callbacks(save_path):
    save_checkpoint = ModelCheckpoint(
        filepath=save_path, monitor='val_loss', save_best_only=True)
    early_stopping = EarlyStopping(
        monitor='val_loss', patience=4, verbose=1, min_delta=1e-4, mode='min')
    Callbacks = [save_checkpoint, early_stopping]
    return Callbacks
