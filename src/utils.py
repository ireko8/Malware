import resource
from multiprocessing import Pool
from pathlib import Path
from datetime import datetime
from shutil import copytree
import random
import numpy as np
import torch
import feather
import pandas as pd
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
from tqdm import tqdm
# from config import conf


def set_ulimit(limit):
    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)
    resource.setrlimit(resource.RLIMIT_NOFILE, (limit, rlimit[1]))


def now():
    return datetime.now().strftime("%Y_%m_%d_%H_%M_%S")


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = False
    torch.manual_seed(seed)


def load_feather(test=False):
    if test:
        path = 'features/test.ftr'
    else:
        path = 'features/train.ftr'

    df = feather.read_dataframe(path)
    return df


def load_fs(fdir, conf, test=False):
    if test:
        path = f'features/NN/test_{fdir}'
    else:
        path = f'features/NN/train_{fdir}'
    fs_list = Path(path).glob('*.ftr')
    fs = list()
    features = conf.num_cols + conf.cat_cols
    for c in tqdm(fs_list):
        if c.stem in features:
            f = feather.read_dataframe(c)
            fs.append(f)
    df = pd.concat(fs, axis=1)
    return reduce_mem_usage(df)


def load_fs_tosh(fdir, conf, test=False):
    if test:
        path = f'features/NN/test_{fdir}'
    else:
        path = f'features/NN/train_{fdir}'
    fs_list = Path(path).glob('*.ftr')
    fs = list()
    num_cols = sum([v for v in conf.num_cols.values()], [])
    features = num_cols + conf.cat_cols

    for c in tqdm(fs_list):
        if c.stem in features:
            f = feather.read_dataframe(c)
            fs.append(f)
    df = pd.concat(fs, axis=1)
    return reduce_mem_usage(df)


def load_fs_ae(fdir, conf, test=False):
    if test:
        path = f'features/NN/test_{fdir}'
    else:
        path = f'features/NN/train_{fdir}'
    fs_list = Path(path).glob('*.ftr')
    fs = list()
 
    for c in tqdm(fs_list):
        if c.stem in conf.num_cols:
            f = feather.read_dataframe(c)
            fs.append(f)
    df = pd.concat(fs, axis=1)
    return reduce_mem_usage(df)


def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'int' or col_type == 'float':
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    # print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    # print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return df


def plot_importance(fi, fn, img_path):
    if isinstance(fi, list):
        fi_mean = np.mean(fi, axis=0)
        fi_std = np.std(fi, axis=0)
        fi_df = pd.DataFrame(np.array([fi_mean, fi_std, fn]).T,
                             columns=['feature_importance', 'fi_std', 'name'])
    
        fi_df.feature_importance = fi_df.feature_importance.astype('float32')
        fi_df.fi_std = fi_df.fi_std.astype('float32')
        
        fi_df = fi_df.sort_values('feature_importance', ascending=True)
        fi_df.plot.barh(y='feature_importance', x='name', xerr='fi_std')
        fi_df.to_csv(img_path / 'fi.csv', index=False)
    else:
        fi_df = pd.DataFrame(np.array([fi, fn]).T,
                             columns=['feature_importance', 'name'])
        fi_df.feature_importance = fi_df.feature_importance.astype('float32')
        fi_df = fi_df.sort_values('feature_importance', ascending=True)
        fi_df.plot.barh(y='feature_importance', x='name')
        fi_df.to_csv(img_path / 'fi.csv', index=False)

    plt.savefig(img_path / 'fi_plot.pdf')
    return fi_df


def rank_average(prob_list):
    avg = np.zeros_like(prob_list[0])
    for p in prob_list:
        p = pd.Series(p)
        avg += p.rank() / p.shape[0]
    avg /= len(prob_list)
    return avg


def count_parameter(model):
    return sum(p.numel() for p in model.parameters())


def get_lr(optimizer):
    lr = list()
    for param_group in optimizer.param_groups:
        lr.append(param_group['lr'])
    if len(lr) == 1:
        return lr[0]
    else:
        return lr


def copy_script(cv_path):
    copytree('src', cv_path / 'src')


def compare_submission(preds, s):
        other_pred = pd.read_csv(s, compression='gzip')
        corr, _ = spearmanr(other_pred.HasDetections,
                            preds)
        print(s, corr)


def make_submission(preds, fname, compare=True):
    sub = pd.read_csv('input/sample_submission.csv')
    if compare:
        subs = Path('submissions').glob('*.csv.gz')
        subs = sorted(list(subs))
        arg_list = [(preds, s) for s in subs]
        with Pool(16) as p:
            p.starmap(compare_submission, arg_list)
    sub['HasDetections'] = preds
    sub.to_csv(fname, index=False, compression='gzip')


def remove_all_zero(p):
    v = np.sum(p, axis=0)
    p = p[:, v != 0]
    return p

